<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Deep Learning on Atishay Jain</title><link>https://atishay.me/series/deep-learning/</link><description>Recent content in Deep Learning on Atishay Jain</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><managingEditor>contact@atishay.me (Atishay Jain)</managingEditor><webMaster>contact@atishay.me (Atishay Jain)</webMaster><lastBuildDate>Thu, 21 Sep 2017 00:00:00 +0000</lastBuildDate><atom:link href="https://atishay.me/series/deep-learning/index.xml" rel="self" type="application/rss+xml"/><item><title>Easy Deep Learning Part X</title><link>https://atishay.me/blog/2017/09/21/deep-learning-part-10/</link><pubDate>Thu, 21 Sep 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/09/21/deep-learning-part-10/</guid><description>Tips from the experts This is the tenth part of a multi-part series on deep learning. You should start with Part 1.
Recap So far we have looked at dense and convolutional neural networks. Then we tried our hands with transfer learning, that is using a pre-trained complicated model and tuning it to our different data set and getting wonderful results for that data set.
In this post we look at the identifying characteristics and new techniques that we can pick up from the best papers in the image-net challenge.</description></item><item><title>Easy Deep Learning Part IX</title><link>https://atishay.me/blog/2017/09/19/deep-learning-part-9/</link><pubDate>Tue, 19 Sep 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/09/19/deep-learning-part-9/</guid><description>Using the model library This is the ninth part of an intended multi-part series on deep learning. You should start with Part 1.
Recap In the previous posts we got to understand the intuitions behind regular(dense) and convolutional neural networks. Now it is time to see why we could skip all that.
Transfer Learning Building neural networks from scratch is a great way to learn the details of how they work, but there is a much better way if we are looking to apply the principles in practice.</description></item><item><title>Easy Deep Learning Part VIII</title><link>https://atishay.me/blog/2017/09/18/deep-learning-part-8/</link><pubDate>Mon, 18 Sep 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/09/18/deep-learning-part-8/</guid><description>ConvNets on CIFAR 10 This is the eighth part of a multi-part series on deep learning. You should start with Part 1.
Recap In the previous posts we introduced neural networks, what they are and how they work as well as convolutional neural networks that provide methods to look at local information and generate the next layer by collecting information from a set of 9 neighbors.
CIFAR-10 We have played a lot with MNIST and now it is time to introduce a much more complicated CIFAR data set.</description></item><item><title>Easy Deep Learning Part VII</title><link>https://atishay.me/blog/2017/08/30/deep-learning-part-7/</link><pubDate>Wed, 30 Aug 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/08/30/deep-learning-part-7/</guid><description>Convolutional Neural Networks This is the seventh part of an intended multi-part series on deep learning. You should start with Part 1.
Recap By now you should be comfortable with what a neuron is, what a neural network (or a stack of neuron means). We have so far described a neuron to represent g(AX + b) which was chained together in multiple layers that created a deep network. Next we talked about concepts like dropout, different activation function, regularization and different initialization to get better results.</description></item><item><title>Easy Deep Learning Part VI</title><link>https://atishay.me/blog/2017/08/29/deep-learning-part-6/</link><pubDate>Tue, 29 Aug 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/08/29/deep-learning-part-6/</guid><description>Contracts, Options and Futures This is the sixth part of an intended multi-part series on deep learning. You should start with Part 1.
Recap In the previous posts we came up with the equation of a neuron to be f(X) = g(Ax + b) and talked about how we can stack one neuron over other to get a chain and make the network deep. We also talked about SGD and how we can slowly change our random parameters to get to the correct answer by going in the direction of the gradient.</description></item><item><title>Easy Deep Learning Part V</title><link>https://atishay.me/blog/2017/08/24/deep-learning-part-5/</link><pubDate>Thu, 24 Aug 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/08/24/deep-learning-part-5/</guid><description>Lets go Deep This is the fifth part of an intended multi-part series on deep learning. You should start with Part 1.
Recap In the previous sections we defined our deep learning task of identifying the contents of an image:
g(AX + b) = Probability of being a content type where X is the huge matrix that makes up the image and A are the weights, b the biases and g is the function that converts scores into probabilities like softmax.</description></item><item><title>Easy Deep Learning Part IV</title><link>https://atishay.me/blog/2017/08/22/deep-learning-part-4/</link><pubDate>Tue, 22 Aug 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/08/22/deep-learning-part-4/</guid><description>Working code This is the fourth part of an intended multi-part series on deep learning. You should start with Part 1.
Recap In the previous sections we defined our deep learning task of identifying the contents of an image:
g(AX + b) = Probability of being a content type where X is the huge matrix that makes up the image and A are the weights, b the biases and g is the function that converts scores into probabilities like softmax.</description></item><item><title>Easy Deep Learning Part III</title><link>https://atishay.me/blog/2017/08/21/deep-learning-part-3/</link><pubDate>Mon, 21 Aug 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/08/21/deep-learning-part-3/</guid><description>Training &amp;amp; Testing This is the third part of a multi-part series on deep learning. You should start with Part 1.
Recap In the previous sections we defined our deep learning task of detecting if an image is of a cat via the equation:
g(AX + b) = Probability of being a cat where X is the huge matrix that makes up the image and A are the weights, b the biases and g is the function that converts scores into probabilities like softmax.</description></item><item><title>Easy Deep Learning II</title><link>https://atishay.me/blog/2017/08/18/deep-learning-part-2/</link><pubDate>Fri, 18 Aug 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/08/18/deep-learning-part-2/</guid><description>Finishing Problem Definition This is the second part of an multi-part series on deep learning. You can read Part 1 here. In this post we complete the minimum equation of an image classifier that takes an image and gives us the likelihood that it is that of a cat.
Recap In the previous section we defined our deep learning task of detecting if an image is of a cat via the equation:</description></item><item><title>Easy Deep Learning Part I</title><link>https://atishay.me/blog/2017/08/16/deep-learning-part-1/</link><pubDate>Wed, 16 Aug 2017 00:00:00 +0000</pubDate><author>contact@atishay.me (Atishay Jain)</author><guid>https://atishay.me/blog/2017/08/16/deep-learning-part-1/</guid><description>Defining the problem This is the first part of a multi-part series on deep learning as I sort my thoughts around the learnings in the last 1 year and put them here for my memories and help fellow humans get rid of the fear of the mechanical brain. In this part we describe what deep learning is and start formulating an equation for the image classification problem in deep learning.</description></item></channel></rss>